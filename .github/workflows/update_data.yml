name: Scheduled Data Update

on:
  # 設定排程：每天格林威治時間 00:00 執行 (台灣時間早上 8 點)
  schedule:
    - cron: '0 0 * * *'
  # 也可以手動運行
  workflow_dispatch:

jobs:
  run_scraper_and_update_data:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository code
        uses: actions/checkout@v4

      # 步驟 A: 安裝 Chrome/Chromedriver
      - name: Install necessary tools (Chromium)
        run: |
          sudo apt-get update
          # 安裝無頭 Chrome 瀏覽器 (Headless)
          sudo apt-get install chromium-browser -y
          # 安裝 ChromeDriver (讓 Selenium 控制它)
          sudo apt-get install chromium-chromedriver -y
          
      - name: Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies (From requirements.txt)
        run: |
          pip install -r requirements.txt
          pip install geopy pandas beautifulsoup4

      # 步驟 B: 執行爬蟲和地理編碼
      - name: Run Scraper and Geocoding
        run: |
          # 為了讓 Selenium 找到 Chromedriver，我們需要設定環境變數
          export CHROME_BIN=/usr/bin/chromium-browser
          export CHROMEDRIVER_PATH=/usr/lib/chromium-browser/chromedriver

          # 執行爬蟲，這次它應該能找到 Chromedriver
          python auto_scraper.py 
          
          # 執行地理編碼
          python geocode.py

      # 步驟 C: 提交更新的資料
      - name: Commit and Push new data to GitHub
        run: |
          git config user.name 'GitHub Actions'
          git config user.email 'actions@github.com'
          # 檢查是否有任何變動 (避免錯誤)
          git add MOHW_counseling_data_FINAL.csv
          git commit -m "SCHEDULER: Auto update counseling data with latest availability." || echo "No changes to commit"
          # 強制推送到倉庫
          git push
