name: Scheduled Data Update

on:
  # 設定排程：每天格林威G 治時間 00:00 執行 (台灣時間早上 8 點)
  schedule:
    - cron: '0 0 * * *'
  # 允許手動運行 (用於測試)
  workflow_dispatch:

jobs:
  run_scraper_and_update_data:
    runs-on: ubuntu-latest # 使用最新的 Linux 環境
    
    steps:
      # 步驟 1: 取得您的程式碼
      - name: Checkout repository code
        uses: actions/checkout@v4

      # 步驟 2: 安裝 Chrome 瀏覽器 (雲端環境需要)
      - name: Install necessary tools (Chromium)
        run: |
          sudo apt-get update
          sudo apt-get install chromium-browser -y
          sudo apt-get install chromium-chromedriver -y
          
      # 步驟 3: 設定 Python 環境
      - name: Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'

      # *** 關鍵修正：手動安裝所有需要的 Python 套件 ***
      - name: Install ALL Python dependencies
        run: |
          # 不再依賴 requirements.txt，我們明確安裝所有東西
          pip install pandas
          pip install selenium
          pip install beautifulsoup4
          pip install geopy
          pip install streamlit
          pip install folium
          pip install streamlit-folium
          
      # 步驟 4: 執行您的爬蟲和地理編碼
      - name: Run Scraper and Geocoding
        run: |
          echo "--- Starting Scraper ---"
          # 執行爬蟲 (它會使用 /usr/bin/chromedriver)
          python auto_scraper.py 
          
          echo "--- Starting Geocoding ---"
          # 執行地理編碼
          python geocode.py

      # 步驟 5: 將新資料提交回 GitHub
      - name: Commit and Push new data to GitHub
        run: |
          git config user.name 'GitHub Actions'
          git config user.email 'actions@github.com'
          # 檢查是否有任何變動 (避免錯誤)
          git add MOHW_counseling_data_FINAL.csv
          git commit -m "SCHEDULER: Auto update counseling data with latest availability." || echo "No changes to commit"
          git push
